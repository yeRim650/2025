# 자료구조
### 시간복잡도와 공간복잡도
<details>
<summary>Big-O, Big-Theta, Big-Omega 에 대해 설명해 주세요.</summary>
<div>

- Big-O 표기법은 최악의 경우 시간 복잡도를 나타내며, 알고리짐의 성능을 이해하는데 중요한 도구
- Big-Theta 표기법은 알고리즘의 평균적인 경우의 시간 복잡도
- Big Omega 표기법은 최선의 경우의 시간 복잡도

</div>
</details>
<details>
<summary>다른 것을 사용하지 않고, Big-O를 사용하는 이유가 있을까요?</summary>
<div>

- 알고리즘의 최악의 경우를 생각해야 함

</div>
</details>
<details>
<summary>O(1)은 O(N^2) 보다 무조건적으로 빠른가요?</summary>
<div>

- O(1)는 데이터 크기와 상관없이 일정한 시간이 걸리는 알고리즘
- O(N^2)는 데이터가 많아질 수록 처리시간이 급수적으로 늘어나는 알고리즘
- 데이터가 적다면 일정 수준까지 O(N^2)가 빠르거나 차이 적을 수는 있지만 데이터 양이 많아진다면 차이는 유의미하게 벌어져 O(1)가 훨씬 빠르다는걸 알 수 있다.

</div>
</details>

### 링크드 리스트에 대해 설명해 주세요.
<details>
<summary>일반 배열과, 링크드 리스트를 비교해 주세요.</summary>
<div>

- 배열은 정적자료 구조이며 index로 임의의 접근이 가능하다는 장점이 있어 접근과 탐색이 용이
- 링크드 리스트는 동적 자료구조로 크기를 정할 필요 없으며 노가 존재하여 노드 안에 데이터가 있고, 다음 데이터를 가르키는 주소를 가짐
- 크기의 제한이 없어 데이터 추가, 삭제가 자유롭지만 임의로 접근하는 것이 불가능하여 데이터를 탐색할 때 순차적으로 접근해야함

</div>
</details>

<details>
<summary>링크드 리스트를 사용해서 구현할 수 있는 다른 자료구조에 대해 설명해 주세요.</summary>
<div>

- 링크드 리스트로 구현할 수 있는 자료구조는 대부분 만들 수 있다. 대표적으로 스택이나 큐가 있다.
- 스택 : Last In Fist Out을 하는 자료구조
- 큐 : First In Fist Out을 하는 자료구조

</div>
</details>

### 스택과 큐에 대해서 설명해 주세요.
<details>
<summary>스택 2개로 큐를, 큐 2개로 스택을 만드는 방법과, 그 시간복잡도에 대해 설명해 주세요.</summary>
<div></div>
</details>
<details>
<summary> 시간복잡도를 유지하면서, 배열로 스택과 큐를 구현할 수 있을까요?</summary>
<div></div>
</details>
<details>
<summary>Prefix, Infix, Postfix 에 대해 설명하고, 이를 스택을 활용해서 계산/하는 방법에 대해 설명해 주세요.</summary>
<div>

- Prefix : 연산자를 먼저 표시하고 연산에 필요한 피연산자를 나중에 표기하는 방법이다. (+AB)
- Infix : 연산자를 두 피연산자 사이에 표기하는 방법으로 가장 일반적으로 사용되는 표현 방법이다. (A+B)
- Postfix : 피연산자를 먼저 표시하고 연산자를 나중에 표시하는 방법이다. (AB+)

</div>
</details>
<details>
<summary>Deque는 어떻게 구현할 수 있을까요?</summary>
<div></div>
</details>
<details>
<summary>(C++ 한정) Deque의 Random Access 시간복잡도는 O(1) 입니다. 이게 어떻게 가능한걸까요?</summary>
<div></div>
</details>

### 해시 자료구조에 대해 설명해 주세요.
> 해시란 여러 길이의 데이터를 효율적으로 관리하기 위해 해시 함수를 통해 일정한 길이의 값으로 매핑하는 것을 말한다.
<details>
<summary>값이 주어졌을 때, 어떻게 하면 충돌이 최대한 적은 해시 함수를 설계할 수 있을까요?</summary>
<div>

- Division Method :
값을 버킷 사이즈로 나누어 나머지를 전체 버킷사이즈에서 뺀값으로 사용, 이 때 버킷사이즈는 소수를 사용하고 2의 제곱수와 먼 값을 사용하는 것이 좋다.
- Digit Folding :
값이 문자열일 때 아스키코드로 바꿔 합한 값을 사용, 이때 버킷사이즈를 넘어갈 수 있기 때문에 Divison Method를 함께 사용
- Multiplication Method :
floor(k*A mod 1) * m의 식을 사용한다. A는 0과 1사이의 실수 m은 2의 제곱수를 사용한다.
- Univiersal Hashing :
여러 해시함수를 무작위로 사용한다.
</div>
</details>

<details>
<summary>해시값이 충돌했을 때, 어떤 방식으로 처리할 수 있을까요?</summary>
<div>

- Chaning :
연결리스트를 사용해 중복된 값을 저장한다.
- Open Addressing :
인덱스가 중복되었다면 다른 빈 인덱스를 찾아 저장하는 방법
    - linear probing :
가장 가까운 빈 인덱스를 사용
    - quadratic probing :
2의 제곱수로 탐색하여 빈 인덱스를 사용
    - double hashing probing :
빈 인덱스를 찾을 때 까지 해시 함수를 사용하여 탐색
</div>
</details>

<details>
<summary>Java에서는 해시 충돌을 어떻게 해결했을까?</summary>
<div>

- jdk7까지는 linked list를 사용한 separate chaning을 활용.
- jdk8에서 linked list와 red black tree를 혼용한 separate chaining을 활용.
- 충돌을 한 key-value쌍이 적을때는 linked list로 작동을 한다.
- 충돌을 한 key-value쌍이 특정 임계치에 도달하면 red black tree로 작동을 한다.
</div>
</details>

<details>
<summary>Double Hashing 의 장점과 단점에 대해서 설명하고, 단점을 어떻게 해결할 수 있을지 설명해 주세요.</summary>
<div>

- 장점 : 클러스터링에 영향을 받지 않음
- 단점 : 연산량이 많음, 캐쉬의 효율이 가장 좋지 않음
- primary clustering : 특정 영역에 원소가 몰리는 현상
- secondary clustering : 여러 개의 원소가 동일한 초기 해시 값을 갖는 현상
</div>
</details>

<details>
<summary>다른 자료구조와 비교하여, 해시 테이블은 멀티스레드 환경에서 심각한 수준의 Race Condition 문제에 빠질 위험이 있습니다. 성능 감소를 최소화 한 채로 해당 문제를 해결할 수 있는 방법을 설계해 보세요.</summary>
<div>

- Race Condition(경쟁 상태) 혹은 경쟁 조건이라고도 불리우며 로 둘 이상의 스레드, 프로세스 그외 작업들이 공유 자원(변수, 메모리, 파일 등)에 대해 동시에 접근할 때 누가 언제 데이터를 읽거나 쓰느냐에 따라 결과가 달라질 수 있는 문제것입니다
- 해결방법
    - 상호배제(Mutual exclusion) : 스레드가 공유 변수 또는 공유 스레드를 사용하는 경우 다른 스레드가 동일한 작업을 수행하지 못하도록 배제한다.
    - 프로세스 동기화(Synchronize the process) : 한번에 하나의 프로세스만 공유데이터에 액세스할 수 있도록 한다.
</details>

### 트리와 이진트리, 이진탐색트리에 대해 설명해 주세요.

> 트리 : 데이터를 계층적으로 표현, 사클을 이루지 않고 구성된 데이터 ( 비선형 자료구조, 무방향 비순환 그래프, 계층형 자료구조 )  
> 이진트리 : 자식 노드가 최대 두개인 노드들로 구성된 트리  
> 이진탐색트리 : 모든 왼쪽 자식의 값이 루트나 부모보다 작고, 모든 오른쪽 자식의 값이 루트나 부모보다 큰 값을 가진다.

<details>
<summary>그래프와 트리의 차이가 무엇인가요?</summary>
<div>

- 그래프는 부모-자식 관계가 없지만 트리는 계층형 구조를 가지고 있다
- 들다 노드와 노드간을 연결하는 간선으로 구성된 자료구조
- 트리는 한개의 경로만 가지고 그래프는 두개이상의 경로가 가능
</div>
</details>
<details>
<summary>이진탐색트리에서 중위 탐색을 하게 되면, 그 결과는 어떤 의미를 가지나요?</summary>
<div>

- 중위 순회 : 왼쪽 자식, 노드, 오른쪽 자식 순서로 방문하는 순회 방법
- 트리에 있는 데이터가 정렬된 순서대로 결과가 나옵니다. 
- 오름차순의 형태

</div>
</details>
<details>
<summary>이진탐색트리의 한계점에 대해 설명해주세요.</summary>
<div>

- 편향트리의 경우 링크드 리스트와 같은 형태가 된다.
- 균형 잡힌 트리가 만들어지도록 조건을 설정해야 한다.
</div>
</details>
<details>
<summary>이진탐색트리의 주요 연산에 대한 시간복잡도를 설명하고, 왜 그런 시간복잡도가 도출되는지 설명해 주세요.</summary>
<div>

- 찾고자하는 값과 현재 루트 노드의 값 비교 ->타겟 값이 더 크다면 오른쪽 서브 트리로 ->타겟 값이 더 작다면 왼쪽 서브 트리로
- 포화 트리인 경우 최악의 상황에서는 트리의 높이만큼 탐색하기 때문에 O(logn)
- 편향 트리인 경우 트리의 높이는 n이 되므로, O(n)의 시간 복잡도를 가진다
</div>
</details>
<details>
<summary>이진탐색트리의 값 삽입, 삭제 방법에 대해 설명하고, 어떤식으로 값을 삽입하면 편향이 발생할까요?</summary>
<div>

- 삽입 연산은 새로운 노드를 삽입할 위치를 찾는 과정과 그 위치에 노드를 추가하는 과정으로 구성됨
- 새로운 노드를 삽입할 위치를 찾아야 하기 때문에 탐색과 동일한 시간복잡도를 가짐
- 포화트리 편향트리 O(logn)
- 자식 두 개인 노드를 삭제하는 경우가 최악 시간복잡도
- 삭제할 노드를 찾고 해당 자식 노드도 찾아야 하는 경우이고 최대 시간복잡도는 O(logn)으로 동일
- 오름차순, 내림차순으로 값을 삽입하면 편향이 발생함
</div>
</details>
<details>
<summary>이진탐색트리와 동일한 로직을 사용하면, 삼진탐색트리도 정의할 수 있을까요? 안 된다면, 그 이유에 대해 설명해 주세요.</summary>
<div>

- 동일한 로직을 사용하면 삼진탐색트리를 정의할 수 없음
- 삼진탐색트리는 이진탐색트리와 달리, 3개 구간으로 나눠지므로 중간 자식의 값을 지정하는 방법을 어떻게 하냐에 딸라 달라지기 때문에 정의할 수 없음

</div>
</details>

### 힙에 대해 설명해 주세요.
> 무엇인가를 차곡차곡 쌓아올린 더미  
> 완전 이진 트리의 일종으로 우선순위 큐를 위하여 만들어진 자료구조  
> 부모의 값은 항상 자식(들)의 값보다 크거나(최대 힙[max heap]), 작아야(최소 힙[min heap])하는 규칙
<details>
<summary>힙을 배열로 구현한다고 가정하면, 어떻게 값을 저장할 수 있을까요?</summary>
</details>
<details>
<summary>힙의 삽입, 삭제 방식에 대해 설명하고, 왜 이진탐색트리와 달리 편향이 발생하지 않는지 설명해 주세요.</summary>
</details>
<details>
<summary>힙 정렬의 시간복잡도는 어떻게 되나요? Stable 한가요?</summary>
</details>

### BBST (Balanced Binary Search Tree) 와, 그 종류에 대해 설명해 주세요.
> 균형 이진 탐색 트리는 노드의 삽입과 삭제가 발생할 때 균형을 유지하도록 하는 트리  
> 특정 노드에 대해 최악의 경우에도 탐색, 삽입, 삭제 연산이 O(logn)의 시간 복잡도를 가지도록 설계된 이진 탐색 트리  
> 균형 이진 탐색 트리는 이진 탐색 트리의 한 종류  
> AVL 트리는 각 노드의 왼쪽 서브트리와 오른쪽 서브트리의 높이 차이가 최대 1이 되도록 유지하는 이진 탐색 트리  
> 레드-블랙 트리는 노드가 레드 또는 블랙 색상을 가지며, 몇 가지 색상 속성을 통해 트리의 균형을 유지하는 이진 탐색 트리  
> B-트리는 노드에 여러 키를 저장하며, 모든 리프 노드가 동일한 깊이를 가지도록 유지  
> 2-3-4 트리는 B-트리의 특별한 경우로, 각 노드가 2개, 3개 또는 4개의 자식을 가질 수 잇는 균형 잡힌 트리
<details>
<summary>Red Black Tree는 어떻게 균형을 유지할 수 있을까요?</summary>
<div>

- 리프노드에서 루트노드까지 가는 경로에서 만나는 검은색 노드의 개수가 같다.
</div>
</details>
<details>
<summary>Red Black Tree의 주요 성질 4가지에 대해 설명해 주세요.</summary>
<div>

- 루트 노드는 검은색이다.
- 모든 리프(null lif)는 검은색이다.
- 빨간색 노드의 자식은 검은색이다.
- 모든 리프노드에서 black depth 는 같다.
</div>
</details>
<details>
<summary>2-3-4 Tree, AVL Tree 등의 다른 BBST 가 있음에도, 왜 Red Black Tree가 많이 사용될까요?</summary>
<div>

- 삽입, 삭제 작업 시 균형을 맞추기 위한 작업 횟수가 적다.
- 각 노드당 색깔을 표현하는 데 단 1bit의 저장공간만 필요하다.
- 언제 회전에 의해 균형을 잡아햐 하는지가 쉽게 판별된다.
- 이진 탐색 트리의 함수를 거의 그대로 사용한다.
</div>
</details>

### 정렬 알고리즘에 대해 설명해 주세요.
> 컴퓨터 과학과 수학에서 원소들을 번호순이나 사전 순서 등으로 정렬하는 알고리즘
<details>
<summary>Quick Sort와 Merge Sort를 비교해 주세요.</summary>
<div>

* 퀵 정렬 : 분할 정복 (Devide and Conquer) 기법과 재귀 알고리즘을 이용한 정렬 알고리즘
* 병함 정렬 : 퀵 정렬과 마찬가지로 분할 정복 방법을 통해 정렬이다. 퀵 정렬과 비슷하지만 안정 정렬

* 부분 배열의 구획 : 나뉘어진 배열은 여러 비율로 나뉜다. vs	배열은 항상 반으로 나뉜다.
* 최악의 경우 시간복잡도 : O(n^2) vs O(nlogn)
* 사용 용도 : 작은 크기의 배열에서 잘 동작 vs 어떤 크기의 Dataset에서도 적절히 동작
* 효율성 : 작은 크기 Dataset에서는 병합 정렬보다 빠르다. vs 큰 Dataset에서는 퀵 정렬보다 빠르다.
* 정렬 방식 : 내부 정렬 vs 외부 정렬
* 별도 저장 공간 : 불 필요 vs 필요
* Stable : X(그러나 구현 방식에 따라 가능) vs O
</div>
</details>

<details>
<summary>Quick Sort에서 O(N^2)이 걸리는 예시를 들고, 이를 개선할 수 있는 방법에 대해 설명해 주세요.</summary>
<div>

* 오름차순 혹은 내림차순으로 정렬되어 파티션이 나뉘지 않는 경우
* 중간값과 맨 앞값을 서로 스왑해주면 어느정도 개선이 가능
</div>
</details>

<details>
<summary>Stable Sort가 무엇이고, 어떤 정렬 알고리즘이 Stable 한지 설명해 주세요.</summary>
<div>

* stable sort란 중복된 값이 있을 시 이 순서가 변경되지 않는 정렬을 의미한다. insertion sort, merge sort, bubble sort, counting sort가 있다.
</div>
</details>

<details>
<summary>Merge Sort를 재귀를 사용하지 않고 구현할 수 있을까요?</summary>
<div>

* 큐를 이용하여 구현할 수 있다.
</div>
</details>

<details>
<summary>Bubble, Selection, Insertion Sort의 속도를 비교해 주세요.</summary>
<div>

* 시간복잡도는 모두 O(n^2)
</div>
</details>

### 그래프 자료구조에 대해 설명하고, 이를 구현할 수 있는 두 방법에 대해 설명해 주세요.
> 그래프란 노드와 edge로 구성되어있는 자료구조이다. 이를 구현하기 위해서는 연결 리스트와 배열을 이용하여 구현할 수 있다.
<details>
<summary>각 방법에 대해, "두 정점이 연결되었는지" 확인하는 시간복잡도와 "한 정점에 연결된 모든 정점을 찾는" 시간복잡도, 그리고 공간복잡도를 비교해 주세요.</summary>
<div>

</div>
</details>

<details>
<summary>정점의 개수가 N개, 간선의 개수가 N^3 개라면, 어떤 방식으로 구현하는 것이 효율적일까요?</summary>
<div>

</div>
</details>

<details>
<summary>사이클이 없는 그래프는 모두 트리인가요? 그렇지 않다면, 예시를 들어주세요.</summary>
<div>

</div>
</details>

<details>
<summary>그래프에서 최단거리를 구하는 방법에 대해 설명해 주세요</summary>
<div>

* 그래프에서 최단거리 알고리즘은 다양하게 있음. 다익스트라 알고리즘, 벨만-포드 알고리즘, BFS 등이 있음
* BFS는 가중치가 모두 없거나 모두 동일한 경우, 다익스트라나 밸만포드의 경우는 가중치가 있는 * 그래프에서 최단거리 구할때 용이함. 벨만포드는 특히 음의 가중치를 처리할 때 유용함.
</div>
</details>

### 그래프에서, 최단거리를 구하는 방법에 대해 설명해 주세요.
> 다익스트라 (Dijkstra) 알고리즘(가중치가 있다)  
> 벨만-포드 (Bellman-Ford) 알고리즘(가중치가 있다)  
> 플로이드-워셜 (Floyd-Warshall) 알고리즘(가중치가 있다)  
> BFS(가중치가 없다)  
> 다익스트라(가중치가 없다)  
<details>
<summary>트리에서는 어떤 방식으로 최단거리를 구할 수 있을까요? (위 방법을 사용하지 않고)</summary>
<div>

* LCA 사용
</div>
</details>
<details>
<summary>다익스트라 알고리즘에서, 힙을 사용하지 않고 구현한다면 시간복잡도가 어떻게 변화할까요?</summary>
<div>

|구현 방식 | 시간복잡도 | 비고|  
|힙 사용 안 함 | O(V2+E)O(V^2 + E)O(V2+E) | 노드 수(V)가 작을 때 적합|  
|힙 (우선순위 큐) 사용 | O((V+E)log⁡V)O((V + E) \log V)O((V+E)logV) | 일반적으로 효율적|
</div>
</details>
<details>
<summary>정점의 개수가 N개, 간선의 개수가 N^3 개라면, 어떤 알고리즘이 효율적일까요?</summary>
<div>

단일 출발점 최단 경로만 구하고 싶다면:  
→ 다익스트라 (힙 미사용도 괜찮음)  
→ 어차피 
𝑂
(
𝑁
3
)
O(N 
3
 )이므로 힙의 장점이 크게 부각되지 않음
모든 정점 쌍 최단 경로가 필요하다면:  
→ 플로이드–워셜이 가독성, 구현 난이도 면에서 우수
</div>
</details>
<details>
<summary>A*는 최단 경로 알고리즘으로, 시작 지점에서 목표 지점까지 가장 비용이 적은 경로를 찾습니다.
핵심 아이디어는 현재까지의 거리 + 앞으로의 예상 거리를 합쳐 우선순위를 계산</summary>
<div>

* A* 알고리즘 : A*는 최단 경로 알고리즘으로, 시작 지점에서 목표 지점까지 가장 비용이 적은 경로를 찾습니다.
* 평가 함수
𝑓(𝑛)=𝑔(𝑛)+ℎ(𝑛)
* g(n): 시작점에서 현재 노드 n까지의 실제 비용
* h(n): 목표까지의 예상 비용 (휴리스틱)
→ 이게 A*의 핵심!  

| 항목         | 다익스트라                          | A* 알고리즘                                      |
|--------------|-------------------------------------|--------------------------------------------------|
| 휴리스틱 사용 | ❌ 사용하지 않음                     | ✅ 사용함                                         |
| 우선순위     | 실제 거리 `g(n)`                    | 실제 거리 `g(n)` + 추정 거리 `h(n)`              |
| 성능         | 최단 경로 보장, 느릴 수 있음        | 최단 경로 보장, 빠를 수 있음                     |
| 최적성       | 항상 최적                           | 휴리스틱이 적절하면 최적                         |
| 사용처       | 일반 그래프 탐색                    | 목표가 명확한 탐색 (예: 지도, 게임 AI 등)        |

A* 알고리즘 특징  
✅ 다익스트라보다 빠를 수 있음  
✅ 목표가 있는 탐색에 특화  
✅ 휴리스틱을 잘 짜야 함  
✅ 최적 경로 보장 가능 (단, 휴리스틱이 낙관적일 때)  
</div>
</details>

<details>
<summary>음수 간선이 있을 때와, 음수 사이클이 있을 때 각각 어떤 최단거리 알고리즘을 사용해야 하는지 설명해 주세요.</summary>
<div>

## 상황별 요약

| 상황                      | 가능한 간선 | 특징                                        | 사용 가능한 알고리즘        |
|---------------------------|--------------|---------------------------------------------|------------------------------|
| ✅ 음수 간선만 있음        | 음수 허용     | 최단 경로는 정의 가능                         | **벨만–포드 알고리즘**         |
| ❌ 음수 사이클 있음        | 음수 사이클   | 최단 경로가 무한히 작아짐 (−∞), 정의 불가능     | **최단 경로 정의 불가, 탐지만 가능** |
| ✅ 양수 간선만 있음        | 양수만 허용   | 가장 일반적인 상황                           | **다익스트라, A\*, BFS 등**     |

### 음수 간선이 있을 때 (음수 사이클은 없음)

- 다익스트라는 사용 불가
  - 이미 방문한 노드가 나중에 더 짧은 거리로 갱신될 수 있어 오류 발생
- **벨만–포드 알고리즘**
  - 시간 복잡도: `O(V * E)`
  - 음수 간선을 허용하면서도 **정확한 최단 거리 계산** 가능
  - 동시에 **음수 사이클 탐지** 기능도 포함됨

---

### 음수 사이클이 있을 때

- **최단 경로가 무한히 작아지기 때문에 정의 자체가 불가능**
  - 예: 어떤 노드에서 출발해 음수 사이클을 계속 돌면 비용이 계속 감소
- **벨만–포드로 탐지 가능**
  - 반복 중에도 거리 갱신이 계속 발생하면 음수 사이클 존재

---

### 양수 간선만 있을 때

- 이 경우는 다익스트라, A\*, BFS (가중치 1) 등 다양한 알고리즘 사용 가능
- 일반적으로 가장 빠르고 효율적인 상황

---
</div>
</details>

### 재귀함수에 대해 설명해 주세요.
* 자기 자신을 다시 호출하는 함수를 말해. 문제를 더 작고 같은 형태의 문제로 나눠서 해결하는 방식
<details>
<summary>재귀 함수의 동작 과정을 Call Stack을 활용해서 설명해 주세요.</summary>
<div>

Step	호출 스택	리턴 값  
1	factorial(3)	
2	factorial(3) → factorial(2)	  
3	factorial(3) → factorial(2) → factorial(1)	  
4	factorial(3) → factorial(2)	1  
5	factorial(3)	2  
6	(empty)	6  
</div>
</details>

<details>
<summary>언어의 스펙에 따라, 재귀함수의 최적화를 진행해주는 경우가 있습니다. 어떤 경우에 재귀함수의 최적화가 가능하며, 이를 어떻게 최적화 할 수 있을지 설명해 주세요.</summary>
<div>

- 꼬리 재귀(Tail Recursion) : 스택을 새로 쌓지 않고 덮어씀(재사) → 메모리 절약, Stack Overflow 방지
- 재귀 → 반복 변환 (직접 최적화)
</div>
</details>

### MST가 무엇이고, 어떻게 구할 수 있을지 설명해 주세요
* *MST (Minimum Spanning Tree)**는:
- **무방향 가중치 그래프**에서
- **모든 정점을 연결**하면서
- **총 간선 가중치의 합이 최소**가 되는
- **사이클이 없는 트리 구조**를 말해

즉, 그래프 내의 모든 노드를 최소 비용으로 하나로 연결하는 "가장 저렴한 네트워크"  
#### MST 구하는 알고리즘 2가지

##### 1. Kruskal's Algorithm (크루스칼)

- 간선 중심 (edge-based)
- **가중치 낮은 간선부터 선택**
- **사이클이 생기지 않도록** 선택 (Union-Find 사용)

```
plaintext
복사편집
1. 간선을 가중치 기준으로 정렬
2. 사이클을 만들지 않도록 하나씩 선택 (Union-Find로 판별)
3. 정점 수 - 1 개의 간선을 선택할 때까지 반복

```

> 구현이 간단하고, 간선이 적은 희소 그래프에 유리
> 

---

##### 2. Prim's Algorithm (프림)

- 정점 중심 (vertex-based)
- **임의의 한 정점에서 시작해서**, 연결된 간선 중 최소 가중치를 계속 선택해 나감
- 마치 **BFS + 우선순위 큐** 느낌

```
plaintext
복사편집
1. 아무 정점이나 시작
2. 현재 정점에서 연결된 가장 작은 가중치 간선을 선택
3. 방문 안 한 정점을 트리에 추가
4. 모든 정점이 포함될 때까지 반복

```

> 간선이 많은 밀집 그래프에 유리 (heap 기반으로 구현하면 빠름)

<details>
<summary>Kruskal 알고리즘에서 사용하는 Union-Find 자료구조에 대해 설명해 주세요.</summary>
<div>

Union-Find(또는 Disjoint Set Union, DSU) 자료구조는 서로소 집합(disjoint sets)을 효율적으로 관리하기 위한 자료구조로, Kruskal 알고리즘에서 사이클 발생 여부를 판별할 때 사용됩니다.

📌 핵심 기능
Union-Find는 두 가지 주요 연산을 지원합니다:

Find(x)

원소 x가 속한 집합의 대표(루트) 원소를 반환합니다.

경로 압축(Path Compression)을 통해 성능을 향상시킬 수 있습니다.

Union(x, y)

x와 y가 속한 두 집합을 하나로 합칩니다.

보통 랭크 기반 합치기(Union by Rank) 또는 크기 기반 합치기(Union by Size) 기법을 사용하여 트리 높이를 최소화합니다.

🔧 Kruskal 알고리즘에서의 역할
Kruskal 알고리즘은 간선의 가중치 기준 오름차순 정렬 후, 하나씩 간선을 선택하면서 사이클이 발생하지 않는 경우 해당 간선을 선택합니다.
이때 두 정점이 **같은 집합(같은 트리)**에 속하면 사이클이 생기므로, Find를 통해 루트가 같은지 확인하고,
루트가 다르면 Union으로 병합합니다.

Find, Union 모두 거의 **O(1)**에 가깝게 동작합니다.

정확하게는 아커만 함수의 역함수인 O(α(N)), 거의 상수 시간입니다.
</div>
</details>

<details>
<summary>Kruskal 과 Prim 중, 어떤 것이 더 빠를까요?</summary>
<div>

| 항목 | **Kruskal** | **Prim** |
| --- | --- | --- |
| 접근 방식 | 간선 중심 (Greedy) | 정점 중심 (Greedy) |
| 자료구조 | **Union-Find (Disjoint Set)** | **Priority Queue (Heap)** |
| 정렬 필요 | 간선 가중치 기준 **정렬 필수** | 정렬 불필요, 대신 **최소힙 사용** |
| 시작 조건 | 시작 정점 **상관 없음** | 시작 정점 **필요함** |
| 사이클 처리 | 사이클 생기면 **건너뜀** | 이미 방문한 정점이면 **건너뜀** |
| 그래프 형태 | **희소 그래프**에 유리함 | **조밀 그래프**에 유리함 |
| 시간복잡도 | `O(E log E)` (정렬 + union-find) | `O(E log V)` (heap 연산 중심) |
</div>
</details>

<details>
<summary>Kruskal 과 Prim 알고리즘을 통해 얻어진 결과물은 무조건 트리인가요? 만약 그렇다면 증명해 주세요. 그렇지 않다면, 반례를 설명해 주세요.</summary>
<div>

### 1. **Kruskal 알고리즘 결과는 트리다** — 증명

- 사이클이 생기지 않도록 Union-Find로 체크하며 간선을 추가합니다.
- 모든 정점이 연결될 때까지 간선을 추가하며, 총 `V-1`개의 간선이 선택됩니다.
- 사이클이 없고 정점이 모두 연결됐고 간선이 `V-1`개 → 트리의 정의와 동일합니다.

### ✅ 요약

- 간선 추가 시 사이클 X
- 모든 정점 연결
- 간선 개수 = `V-1`
→ **결과는 트리**

---

### 2. **Prim 알고리즘 결과도 트리다** — 증명

- 시작 정점에서 시작해서, 항상 가장 비용이 적은 간선 하나씩 추가
- 이미 포함된 정점으로 가는 간선은 건너뜀 → 사이클 생기지 않음
- `V`개의 정점을 연결할 때까지 반복하므로, 간선은 `V-1`개

### ✅ 요약

- 방문한 정점 제외하고 확장 → 사이클 X
- 모든 정점 연결
- 간선 개수 = `V-1`
→ **결과는 트리**

</div>
</details>

#### Thread Safe 한 자료구조가 있을까요? 없다면, 어떻게 Thread Safe 하게 구성할 수 있을까요?

<details>
<summary></summary>
<div>

### ✅ 1. Java에서 제공하는 대표적인 Thread-safe 자료구조

### 🔹 `java.util.concurrent` 패키지 기반

- **`ConcurrentHashMap`**
    
    → HashMap의 thread-safe 버전입니다.
    
    → Segment 기반 락 → JDK 8 이후에는 bucket-level 락 또는 CAS 사용.
    
- **`CopyOnWriteArrayList`**, **`CopyOnWriteArraySet`**
    
    → 변경이 거의 없고, 읽기 위주인 경우에 적합.
    
    → 내부적으로 쓰기 시 전체 복사(Copy on Write).
    
- **`BlockingQueue`** 계열
    
    → `ArrayBlockingQueue`, `LinkedBlockingQueue`, `PriorityBlockingQueue` 등
    
    → 생산자-소비자 패턴에 적합.
    

---

### ✅ 2. 동기화 래퍼 사용 (`Collections.synchronizedXXX`)

Java의 `Collections` 유틸 클래스는 기존 자료구조를 동기화된 버전으로 래핑할 수 있습니다.

```java
java
복사편집
List<String> syncList = Collections.synchronizedList(new ArrayList<>());
Map<String, String> syncMap = Collections.synchronizedMap(new HashMap<>());

```

> ✅ 단점: synchronized 블록 사용 → 성능 저하.
> 
> 
> ✅ **iterator 사용 시 별도 `synchronized` 블록 필요**:
> 

```java
java
복사편집
synchronized (syncList) {
    for (String item : syncList) {
        // 작업 수행
    }
}

```

---

### ✅ 3. 직접 Thread-safe하게 구현하려면?

Thread-safe 하지 않은 자료구조(ex. `ArrayList`, `HashMap`)를 멀티스레드 환경에서 안전하게 사용하려면 **동기화**를 직접 처리해야 합니다.

### 방법 1: `synchronized` 키워드 사용

```java
java
복사편집
public class SafeList {
    private final List<String> list = new ArrayList<>();

    public synchronized void add(String item) {
        list.add(item);
    }

    public synchronized String get(int index) {
        return list.get(index);
    }
}

```

### 방법 2: `ReentrantLock` 사용

```java
java
복사편집
public class SafeList {
    private final List<String> list = new ArrayList<>();
    private final ReentrantLock lock = new ReentrantLock();

    public void add(String item) {
        lock.lock();
        try {
            list.add(item);
        } finally {
            lock.unlock();
        }
    }
}

```

---

### ✅ 상황에 따른 선택 기준

| 상황 | 추천 자료구조 |
| --- | --- |
| 읽기 많은 경우 | `CopyOnWriteArrayList`, `ConcurrentHashMap` |
| 쓰기 많은 경우 | `ConcurrentHashMap`, `BlockingQueue` |
| 간단한 동기화만 필요 | `Collections.synchronizedXXX` |
| 세밀한 락 제어 필요 | 직접 구현 (`ReentrantLock`, `AtomicXXX`) |

---

필요하면 `AtomicInteger`, `AtomicReference`, `StampedLock`, `ReadWriteLock` 같은 더 정밀한 도구도 소개해줄 수 있어요.

혹시 어떤 상황에서 thread-safe가 필요한지 예시나 코드가 있다면, 거기에 딱 맞는 자료구조나 구현 방법을 추천해줄 수도 있어요!

## ✅ 1. 정밀한 동시성 도구들

### 🔸 `AtomicInteger`, `AtomicLong`, `AtomicReference`

- **Lock 없이도 원자적 연산이 가능한 클래스**
- 내부적으로는 **CAS (Compare-And-Swap)** 명령어를 사용함
- 매우 빠름 → 락보다 성능 좋음
- `Unsafe` 클래스의 low-level 기능 활용

```java
java
복사편집
AtomicInteger count = new AtomicInteger(0);

// 스레드 간 안전하게 값 증가
count.incrementAndGet();   // ++count
count.addAndGet(5);        // count += 5
count.compareAndSet(10, 20); // 현재 값이 10이면 20으로 변경

```

---

### 🔸 `AtomicReference<T>`

- 객체 참조 자체를 CAS로 원자적으로 교체
- 예: 다중 스레드 환경에서 객체 상태를 안전하게 교체할 때

```java
java
복사편집
AtomicReference<String> ref = new AtomicReference<>("hello");
ref.compareAndSet("hello", "world");

```

---

### 🔸 `ReadWriteLock` (인터페이스) & `ReentrantReadWriteLock` (구현체)

- **읽기/쓰기 락 분리**
    
    → 읽기 동시 허용, 쓰기는 단독 수행
    
    → 읽기 많고 쓰기 적은 경우 매우 효율적
    

```java
java
복사편집
ReentrantReadWriteLock rwLock = new ReentrantReadWriteLock();
Lock readLock = rwLock.readLock();
Lock writeLock = rwLock.writeLock();

readLock.lock();
// 읽기 작업
readLock.unlock();

writeLock.lock();
// 쓰기 작업
writeLock.unlock();

```

---

### 🔸 `StampedLock` (Java 8~)

- `ReadWriteLock` 보다 더 정밀한 락
- **낙관적 락 (Optimistic Read)** 기능 제공 → 성능 향상
- 쓰기 락은 비슷하지만, 낙관적 읽기는 락 없이 값을 먼저 읽고, 유효성 검사 후 문제 있으면 재시도

```java
java
복사편집
StampedLock lock = new StampedLock();
long stamp = lock.tryOptimisticRead();
int value = this.value; // 먼저 읽기
if (!lock.validate(stamp)) {
    // 중간에 다른 스레드가 썼을 경우, 재시도
    stamp = lock.readLock();
    try {
        value = this.value;
    } finally {
        lock.unlockRead(stamp);
    }
}

```

---

## ✅ 2. Java의 대표적인 Thread-safe 자료구조의 내부 구현 방식

### 🔹 `ConcurrentHashMap`

**Java 8 기준:**

- 버킷 단위 락 (fine-grained lock) + CAS
- `put` 시엔 해당 버킷만 락 (synchronized or CAS)
- `get`은 락 없이 volatile 읽기 → 매우 빠름

```java
java
복사편집
ConcurrentHashMap<String, Integer> map = new ConcurrentHashMap<>();
map.put("key", 1);      // 내부적으로 특정 bin에만 CAS 또는 락
map.get("key");         // 락 없이 읽음

```

### 🔹 `CopyOnWriteArrayList`

- 쓰기 시 전체 배열을 복사 → 새로운 배열 생성 후 참조 교체
- 읽기에는 락이 없음
- 변경이 드물고, 읽기가 많을 때 적합

```java
java
복사편집
CopyOnWriteArrayList<String> list = new CopyOnWriteArrayList<>();
list.add("hello");   // 내부적으로 새로운 배열 생성
list.get(0);         // 락 없이 읽기

```

### 🔹 `BlockingQueue` 계열 (`ArrayBlockingQueue`, `LinkedBlockingQueue` 등)

- `put()`과 `take()`에 내부 `ReentrantLock` 사용
- 생산자-소비자 간 스레드 간 데이터 전달에 적합
- `Condition` 객체로 스레드 대기/알림 처리

```java
java
복사편집
BlockingQueue<String> queue = new ArrayBlockingQueue<>(10);

queue.put("item");      // 큐가 꽉 차면 대기
String item = queue.take(); // 비어 있으면 대기

```

---

## ✅ 요약: 언제 뭘 써야 할까?

| 상황 | 추천 도구 |
| --- | --- |
| 숫자 값 증가/감소 | `AtomicInteger`, `LongAdder` |
| 참조 객체 CAS 변경 | `AtomicReference` |
| 읽기 많은 환경 | `ReadWriteLock`, `StampedLock` |
| 성능 중요, 맵 필요 | `ConcurrentHashMap` |
| 읽기 많은 리스트 | `CopyOnWriteArrayList` |
| 생산자-소비자 큐 | `BlockingQueue` 계열 |

</div>
</details>

<details>
<summary>배열의 길이를 알고 있다면, 조금 더 빠른 Thread Safe 한 연산을 만들 순 없을까요?</summary>
<div>

좋은 질문이에요! 배열의 길이를 알고 있다면, 여러 **스레드 환경(Thread-Safe)**에서도 좀 더 **효율적인 문자열/배열 연산**이 가능해집니다. 일반적으로 **Thread-Safe한 연산은 느려지기 마련**인데, 배열의 길이와 같은 **정적 정보(static information)**를 활용하면 다음과 같은 최적화를 고려할 수 있습니다.

---

## ✅ 기본 아이디어

> "배열의 길이"를 알고 있다는 건, 경계 검사(boundary check), 반복 횟수 등을 **불필요하게 계산하지 않아도 된다**는 뜻입니다. 이를 기반으로 다음을 최적화할 수 있어요:

---

## Thread-Safe 연산을 더 빠르게 만드는 방법들

### 1. **Lock-Free / Wait-Free 알고리즘** 사용
- 길이가 고정된 배열이면, **동기화가 필요 없는 구조**를 설계할 수 있음.
- 예: 고정 크기 Ring Buffer (Lock-Free Queue)
- **CAS (Compare-And-Swap)** 와 같은 원자적 연산으로 동기화

```cpp
// 예시: 배열 길이를 알고 있으므로 mod 연산으로 경계 조절
index = (index + 1) % LENGTH;
```

> 길이가 고정이면, index wrap-around를 안전하게 할 수 있어서 CAS 연산과 잘 어울립니다.

---

### 2. **Pre-allocated Buffer 활용**
- 배열의 크기를 미리 알고 있으면, 동적 메모리 할당을 피하고 **스택 또는 풀 기반** 메모리 사용 가능
- 메모리 할당/해제 자체가 병목인 경우에 큰 차이를 만듬

---

### 3. **경계 검사 제거**
- 배열의 길이를 알고 있다면, 루프마다 `i < arr.length` 같은 조건 검사를 생략하거나 컴파일 타임에 제거 가능
- 특히 JIT 컴파일러나 컴파일러 최적화에서 성능 향상

---

### 4. **배열 분할 후 Thread Local 처리**
- 길이를 알고 있으면 배열을 균등하게 나눠서 각 스레드가 자기 구간만 처리 (Race condition 제거)
- 예:

```python
# 0~9까지의 배열을 3개의 스레드가 처리
# Thread 1: arr[0:3], Thread 2: arr[3:6], Thread 3: arr[6:9]
```

- 이 방법은 thread-safe하면서도 병렬성(Parallelism)을 제대로 살릴 수 있어요.

---

### 5. **읽기 전용(Read-only) 접근이면 락 없이 안전**
- 배열을 수정하지 않고 읽기만 한다면, 모든 스레드가 동시 접근해도 문제가 없습니다.
- 이 경우 별도의 락이나 동기화 없이도 매우 빠르게 연산 가능

---

## ✨ 실전 예시: Java에서의 최적화

```java
public class SafeArray {
    private final int[] array;

    public SafeArray(int size) {
        this.array = new int[size]; // 고정 길이
    }

    public int get(int index) {
        return array[index]; // 길이 고정이면 인라인 최적화 가능
    }

    public void setAtomic(int index, int value) {
        // CAS or AtomicIntegerArray 사용으로 thread-safe
        synchronized (this) {
            array[index] = value;
        }
    }
}
```

---

## 🔚 요약

| 최적화 방식                     | 언제 유용한가                            |
|--------------------------------|------------------------------------------|
| Lock-Free 알고리즘             | 짧은 시간에 반복적으로 접근 시          |
| Pre-allocated Memory           | GC 비용이나 malloc 성능 이슈 있을 때     |
| 경계 검사 제거 (Loop Unrolling 등) | 루프 내부가 병목일 때                   |
| Thread-local 분할              | 병렬 처리가 가능하고 Race Condition이 우려될 때 |
| Read-only 접근                 | 동기화 비용 없이 빠르게 읽고 싶을 때     |

---

</div>
</details>

<details>
<summary>사용하고 있는 언어의 자료구조는 Thread Safe 한가요? 그렇지 않다면, Thread Safe 한 Wrapped Data Structure 를 제공하고 있나요?</summary>
<div>

사용하고 있는 **프로그래밍 언어의 기본 자료구조가 Thread-Safe 한가?** 그리고 그렇지 않다면 **Thread-Safe한 래퍼나 대안이 있는가?**는 다중 스레드 환경에서 매우 중요한 이슈

언어별로 대표적인 자료구조의 **스레드 안전성(Thread Safety)**과 **Thread-Safe 대안**을 정리

---

## ✅ 언어별 자료구조의 Thread Safety 현황

| 언어     | 기본 자료구조 Thread-Safe? | Thread-Safe 래퍼/대안 제공?         |
|----------|----------------------------|--------------------------------------|
| **Java** | ❌ 기본 컬렉션은 대부분 Thread-Unsafe | ✅ `Collections.synchronizedList()`, `ConcurrentHashMap`, `CopyOnWriteArrayList` 등 |
| **C++**  | ❌ `std::vector`, `std::map` 등은 Thread-Unsafe | ❌ STL 자체는 제공 X → `std::mutex` 수동 사용 or `concurrentqueue` 같은 외부 라이브러리 |
| **Python** | ❌ 리스트, 딕셔너리 등 기본 자료구조는 Thread-Unsafe (하지만 GIL 있음) | ✅ `queue.Queue`, `threading.Lock`으로 보호 가능 |
| **Go**   | ❌ 대부분의 자료구조는 Thread-Unsafe | ✅ `sync.Mutex`, `sync.Map`, 채널(goroutine-safe) |
| **Rust** | ✅ by default는 `Send`/`Sync` trait으로 제어됨 | ✅ `Arc<Mutex<T>>`, `RwLock<T>`, `crossbeam`, `dashmap` 등 |
| **Kotlin** | ❌ Java 컬렉션과 동일, Thread-Unsafe | ✅ Java의 Concurrent Collections 사용 가능 or Coroutine + Channel 사용 |
| **JavaScript (Node.js)** | ✅ 단일 스레드 기반이므로 기본 구조는 안전 | ❌ Worker Thread 사용 시 직접 동기화 필요 |

---

## 🔍 주요 언어별 자세한 설명

### **Java**
- `ArrayList`, `HashMap` 등은 기본적으로 **Thread-Unsafe**
- 래핑 방법:
  - `Collections.synchronizedList(list)`
  - `CopyOnWriteArrayList` (읽기 위주 작업에 유리)
  - `ConcurrentHashMap`, `ConcurrentLinkedQueue` 등 병행 컬렉션 제공

---

### **Python**
- 리스트, 딕셔너리는 GIL(Global Interpreter Lock) 덕분에 **동시에 두 스레드가 같은 객체를 수정하지는 못하지만**, 여전히 race condition이 발생할 수 있음
- 안전한 구조:
  - `queue.Queue`: Thread-Safe
  - `multiprocessing.Queue`, `threading.Lock` 등으로 감싸기

---

### **C++**
- STL 컨테이너 (`vector`, `map` 등)는 **Thread-Unsafe**
- 직접 `std::mutex`로 보호하거나 외부 라이브러리 활용
  - 예: `concurrentqueue`, `tbb::concurrent_vector`, `folly::ConcurrentHashMap`

---

### **Go**
- 모든 goroutine은 병렬이지만, 기본 자료구조는 안전하지 않음
- 안전한 방법:
  - `sync.Mutex` 또는 `sync.RWMutex`
  - `sync.Map`: Thread-safe 맵
  - `chan`: 고루틴 간 안전한 통신

---

### **Rust**
- 언어 차원에서 `Send`/`Sync` trait으로 동시성 제어
- `Arc<Mutex<T>>`, `Arc<RwLock<T>>` 등으로 안전하게 공유 가능
- 고성능: `dashmap` (Thread-safe HashMap)

---

## ✨ 결론

- 대부분의 언어에서 **기본 자료구조는 Thread-Safe하지 않음**
- 다만, **Thread-Safe 래퍼나 대안이 제공됨** (혹은 외부 라이브러리 존재)
- 동기화 방법:
  - **락(lock) 기반**: 성능은 느리지만 안전
  - **lock-free/atomic 구조**: 빠르지만 구현 복잡
  - **copy-on-write**: 읽기 위주일 때 유리

---


</div>
</details>

### 문자열을 저장하고, 처리하는 주요 자료구조 및 알고리즘 (Trie, KMP, Rabin Karp 등) 에 대해 설명해 주세요.

<details>
<summary></summary>
<div>

## 🔍 이진 탐색이란?

이진 탐색은 **정렬된 배열**을 전제로, 다음과 같은 방식으로 탐색을 수행합니다:

1. 배열의 **중간 값**을 확인합니다.
2. 찾고자 하는 값이 중간 값보다 작으면 왼쪽 절반에서 다시 탐색합니다.
3. 크면 오른쪽 절반에서 탐색합니다.
4. 일치하면 탐색 종료!

➡️ 매번 **탐색 구간을 반으로 줄여 나가는** 것이 핵심입니다.

---

## 📌 예시

```python
def binary_search(arr, target):
    left, right = 0, len(arr) - 1
    
    while left <= right:
        mid = (left + right) // 2
        
        if arr[mid] == target:
            return mid  # 찾았다!
        elif arr[mid] < target:
            left = mid + 1  # 오른쪽으로 이동
        else:
            right = mid - 1  # 왼쪽으로 이동
            
    return -1  # 못 찾음
```

---

## ⏱ 시간 복잡도 분석

### ⛓ 알고리즘 흐름:
- 매 단계마다 **검색 범위를 절반으로 줄입니다**.
- 만약 원소가 `n`개라면, 가능한 탐색 횟수는 `log₂(n)`번 정도입니다.

예를 들어:
- 16개의 요소 → `log₂(16) = 4`번 비교
- 1,000,000개의 요소 → `log₂(1,000,000) ≈ 20`번 비교!

### ✅ 시간 복잡도:  
- **Best Case**: O(1) → 첫 비교에 찾은 경우  
- **Worst/Average Case**: **O(log n)** (n은 요소의 개수)

---

## 🧠 시간 복잡도 증명 간단히 보기

이진 탐색은 각 단계마다 탐색 범위를 반으로 줄입니다.  
초기 범위: `n`  
1단계 후: `n/2`  
2단계 후: `n/4`  
...  
k단계 후: `n / 2^k`

최대 비교 횟수 `k`는 아래 식을 만족합니다:

```
n / 2^k ≤ 1  
=> 2^k ≥ n  
=> k ≥ log₂(n)
```

즉, **최대 log₂(n)** 번의 비교로도 탐색이 완료됩니다.

---

## 🧩 요약

| 항목 | 설명 |
|------|------|
| 조건 | **정렬된 배열** |
| 핵심 아이디어 | 검색 범위를 절반으로 줄임 |
| 시간복잡도 | **O(log n)** |
| 공간복잡도 | O(1) (반복문), O(log n) (재귀일 경우 호출 스택) |

---

</div>
</details>

<details>
<summary>Lower Bound, Upper Bound 는 무엇이고, 이를 어떻게 구현할 수 있을까요?</summary>
<div>

## 📌 개념 설명

### 🔽 Lower Bound
- **목표값 이상이 처음 나타나는 위치**
- 즉, `arr[i] >= target`을 만족하는 **가장 작은 i**

### 🔼 Upper Bound
- **목표값 초과가 처음 나타나는 위치**
- 즉, `arr[i] > target`을 만족하는 **가장 작은 i**

---

## 🧠 예시로 이해하기

```python
arr = [1, 2, 2, 2, 3, 4, 5]
target = 2
```

- **Lower Bound**: 1 (2가 처음 나타나는 인덱스)
- **Upper Bound**: 4 (2보다 큰 수 3이 처음 나타나는 인덱스)

---

## 🔧 구현 (Python)

```python
def lower_bound(arr, target):
    left, right = 0, len(arr)
    while left < right:
        mid = (left + right) // 2
        if arr[mid] < target:
            left = mid + 1
        else:
            right = mid
    return left

def upper_bound(arr, target):
    left, right = 0, len(arr)
    while left < right:
        mid = (left + right) // 2
        if arr[mid] <= target:
            left = mid + 1
        else:
            right = mid
    return left
```

> 📌 **Note**: `right = len(arr)`으로 설정하는 게 핵심입니다.  
> (`len(arr) - 1`이 아님!)

---

## 🧪 사용 예시

```python
arr = [1, 2, 2, 2, 3, 4, 5]

print(lower_bound(arr, 2))  # 👉 1
print(upper_bound(arr, 2))  # 👉 4
```

---

## 🧩 활용 예시

- **중복 원소 개수 구하기**: `upper_bound - lower_bound`
- **삽입 위치 결정**: lower_bound 사용
- **이분 탐색 문제**: 조건을 만족하는 값 중 최소/최대값 찾기


</div>
</details>

<details>
<summary>이진탐색의 논리를 적용하여 삼진탐색을 작성한다고 가정한다면, 시간복잡도는 어떻게 변화할까요? (실제 존재하는 삼진탐색 알고리즘은 무시하세요!)</summary>
<div>

## 📌 삼진탐색이란? (가정)

- **이진탐색**: 배열을 2등분해서, 절반만 계속 탐색 → O(log₂ n)
- **삼진탐색 (가정)**: 배열을 **3등분**해서, 3분의 1만 탐색

즉, 배열을 다음과 같이 나누고,
```text
[.....|.....|.....]
      m1     m2
```

- `target < arr[m1]` → 왼쪽
- `arr[m1] < target < arr[m2]` → 가운데
- `target > arr[m2]` → 오른쪽

---

## ⏱ 시간복잡도 분석

### 🔢 범위를 1/3로 줄이니까...
매 스텝마다 전체 길이를 **1/3**로 줄입니다.

- 이진탐색: `n → n/2 → n/4 → n/8 ...` → O(log₂ n)
- 삼진탐색: `n → n/3 → n/9 → n/27 ...` → O(log₃ n)

### 따라서,
- **삼진탐색 시간복잡도**: `O(log₃ n)`

---

## 📉 비교: log₂ vs log₃

> 어떤 로그의 밑이 커질수록 **log 값은 작아집니다**

그러나 **밑을 바꾸는 공식**으로 보면:

```text
log₃ n = log₂ n / log₂ 3 ≈ log₂ n / 1.5849
```

즉,
```text
O(log₃ n) ≈ O(0.63 × log₂ n)
```

➡️ **삼진탐색이 조금 더 적은 스텝**을 수행합니다.

---

## 🧠 그럼 왜 대부분 이진탐색을 쓰는가?

좋은 질문! 삼진탐색은 이론상 스텝 수가 약간 줄지만…

- **비교 연산이 더 많아짐**: 한 단계당 2번 비교 (`arr[m1]`, `arr[m2]`)  
  → 이진탐색은 1번 비교
- 실제 성능은 **비교 횟수의 영향이 더 큼**

그래서 **이진탐색이 실질적으로 더 빠름**!

---

## ✅ 정리

| 항목 | 이진탐색 | 삼진탐색 |
|------|----------|-----------|
| 나누는 비율 | 절반 (1/2) | 세 등분 (1/3) |
| 시간복잡도 | O(log₂ n) | O(log₃ n) |
| 비교 횟수 (한 단계) | 1회 | 2회 |
| 실제 성능 | **더 좋음** | 이론상 적은 스텝, but 느릴 수 있음 |

</div>
</details>

<details>
<summary>기존 이진탐색 로직에서 부등호의 범위가 바뀐다면, (ex. <= 라면 <로, <이라면 <= 로) 결과가 달라질까요?</summary>
<div>


## 📌 부등호가 바뀌면 어떤 일이 일어날까?

기본적인 이진탐색 로직을 봅시다:

```python
def binary_search(arr, target):
    left, right = 0, len(arr) - 1
    while left <= right:               # (1)
        mid = (left + right) // 2
        if arr[mid] == target:
            return mid
        elif arr[mid] < target:        # (2)
            left = mid + 1
        else:
            right = mid - 1
    return -1
```

---

## 🎯 부등호 바꾸기 실험

### 1. `while left <= right:` → `while left < right:`

- 이 조건을 바꾸면 **마지막 원소를 탐색하지 못할 수 있음**
- 예: `left == right`일 때 종료되어버림
- 결과적으로 **정확한 검색 실패 가능성**이 생김

### 2. `if arr[mid] < target:` → `if arr[mid] <= target:`

- 이건 subtle한 차이인데, `<=`로 바꾸면:
  - **중복값이 많은 경우**, 오른쪽 범위로 더 쉽게 넘어감
  - 예: `lower_bound` 대신 `upper_bound`처럼 작동할 수 있음
- 반대로 `<`로 하면 **같은 값이 있어도 왼쪽에 머무름** (lower bound 스타일)

### 3. `left = mid + 1` vs `left = mid`

- 이 차이는 `mid`가 이미 비교한 값인데도 다시 포함할지 말지 결정하는 부분
- `left = mid`이면 `while` 조건이 잘못된 경우 무한 루프 가능

---

## 🧠 요약: 부등호가 바뀌면 생기는 영향

| 변경 | 영향 |
|------|------|
| `while left <= right:` → `left < right:` | 마지막 값 탐색 안 하고 종료 |
| `arr[mid] < target` → `arr[mid] <= target` | 탐색 방향/결과 달라짐 (중복 처리 영향) |
| `left = mid + 1` → `left = mid` | **무한 루프 가능성** 있음 |
| `right = mid - 1` → `right = mid` | **무한 루프 가능성** 있음 |

---

## ✅ 실전 팁

- **정확한 값 하나 찾을 땐**: `<=`, `left = mid + 1`, `right = mid - 1`
- **lower_bound 찾을 땐**: `arr[mid] < target` → `left = mid + 1`
- **upper_bound 찾을 땐**: `arr[mid] <= target` → `left = mid + 1`

</div>
</details>

###  그리디 알고리즘과 동적 계획법을 비교해 주세요.

<details>
<summary></summary>
<div>

**그리디 알고리즘(Greedy Algorithm)**과 **동적 계획법(Dynamic Programming, DP)**은 모두 **최적의 해(Optimal Solution)**를 찾기 위한 대표적인 알고리즘 전략

---

## **1. 그리디 알고리즘 (Greedy Algorithm)**

### **핵심 개념**
- **매 단계에서 "현재" 가장 좋아 보이는 선택**을 함
- 전체 최적해를 보장하지는 않지만, 일부 문제에서는 최적해가 됨

### **전제 조건**
- **Greedy Choice Property**: 국소 최적 선택이 전체 최적 결과를 이끈다
- **Optimal Substructure**: 부분 문제의 최적해로 전체 최적해 구성 가능

### **예시 문제**
- 동전 거스름돈 문제 (거스름돈 단위가 잘 짜여 있을 경우)
- 활동 선택 문제
- 크루스칼 알고리즘 (MST)
- 허프만 인코딩

### **장점**
- 구현 간단, 빠름 (보통 O(n log n) 이하)

### **단점**
- 항상 정답을 보장하지 않음

---

## **2. 동적 계획법 (Dynamic Programming)**

### **핵심 개념**
- **복잡한 문제를 작은 부분 문제로 나누고**, 그 결과를 **저장(메모이제이션)**해서 **중복 계산을 줄임**

### **전제 조건**
- **Optimal Substructure**: 부분 문제의 최적해로 전체 최적해 구성 가능
- **Overlapping Subproblems**: 동일한 부분 문제가 여러 번 등장

### **예시 문제**
- 피보나치 수열
- 0/1 배낭 문제
- 최장 공통 부분 수열 (LCS)
- 행렬 곱셈 순서 최적화

### **장점**
- 항상 최적해를 보장
- 다양한 문제에 적용 가능

### **단점**
- 메모리 사용 높을 수 있음
- 구현 복잡도 ↑

---

## **비교 요약표**

| 항목                  | 그리디 알고리즘           | 동적 계획법 (DP)        |
|---------------------|-------------------------|-----------------------|
| 선택 기준             | 현재 최적 선택               | 전체 최적을 위한 모든 경우 고려 |
| 최적해 보장 여부        | 일부 문제만 보장              | 항상 보장                |
| 문제 나누는 방식        | 단계별로 선택                | 부분 문제 저장하며 해결       |
| 시간복잡도            | 보통 O(n log n) 이하         | O(n²), O(n³) 등 많음      |
| 대표 문제 예시         | 동전 문제, MST, 탐욕 선택 문제 | 배낭 문제, LCS, 피보나치 등  |

---

## **예를 들어보자 – 동전 문제**

- 동전 종류: 1원, 3원, 4원  
- 목표: 6원 만들기

### **그리디 선택**:  
- 4 + 1 + 1 → 총 3개 사용

### **동적 계획법 해**:  
- 3 + 3 → 총 2개 (더 나음!)

→ 이처럼 **그리디는 항상 정답을 주진 않음**

---

</div>
</details>

<details>
<summary>
그렇다면, 어떤 경우에 각각의 기법을 사용할 수 있을까요?</summary>
<div>

---

## **1. 그리디 알고리즘 사용 경우**

### **조건**
- **Greedy Choice Property**: **매 순간의 최적 선택**이 결국 **전체 최적해**를 보장
- **Optimal Substructure**: 부분 문제의 최적해로 전체 최적해 구성 가능

### **주로 쓰이는 경우**
- **선택을 점진적으로 쌓아가는 문제**
- **정렬 기반 최적화** (ex. 종료 시간이 빠른 걸 먼저 고르는 활동 선택 문제)

### **대표 예시**
- 최소 신장 트리 (MST): 크루스칼, 프림
- 최단 경로 (음수 간선 없는 경우): 다익스트라
- 활동 선택 문제 (회의실 배정)
- 동전 거스름돈 (단위가 잘 짜여 있을 경우)

---

## **2. 동적 계획법(DP) 사용 경우**

### **조건**
- **Optimal Substructure** + **Overlapping Subproblems**
- 즉, 같은 부분 문제가 계속 반복되고, 그 결과를 **저장해서 재활용**해야 성능 향상

### **주로 쓰이는 경우**
- **조합 최적화 문제** (선택의 조합을 따지는 문제)
- **경로 탐색**, **문자열 비교**, **부분집합 문제**

### **대표 예시**
- 피보나치 수열
- 0/1 배낭 문제
- 최장 공통 부분 수열 (LCS)
- 행렬 곱셈 최적화
- 최소 편집 거리 (edit distance)

---

## **쉽게 판단하는 체크리스트**

| 질문                                                  | Yes → 사용 |
|-------------------------------------------------------|------------|
| "앞 선택이 나중 선택에 영향을 안 줄까?"              | **그리디** 가능 |
| "같은 계산을 여러 번 하고 있나?"                    | **DP** 필요 |
| "최적해를 구하려면, 다양한 선택을 비교해야 하나?"    | **DP** 필요 |
| "항상 가장 좋아 보이는 걸 고르면 결과도 항상 좋은가?" | **그리디** 가능 |

---

## **비유로 설명하자면...**

- **그리디**는: "지금 가장 좋은 걸 선택하자!" → **단기 안목**
- **DP**는: "모든 가능성을 다 따져보자. 기록도 하자!" → **장기 계획**

---

</div>
</details>

<details>
<summary>그렇다면, 동적 계획법으로 풀 수 있는 모든 문제는 재귀로 변환하여 풀 수 있나요?</summary>
<div>

**Yes. 대부분의 동적 계획법(DP) 문제는 재귀로도 풀 수 있어.**  
단, **적절한 최적화가 필요한 경우도 있음.**

---

## **1. 왜 재귀로 풀 수 있을까?**

동적 계획법이 다루는 문제들은 기본적으로 **"부분 문제를 해결해서 전체 문제를 푸는"** 방식이야.  
이는 **자연스럽게 함수가 자기 자신을 호출하는 구조(재귀)**로 변환 가능해.

예를 들어:
```js
function fib(n) {
  if (n <= 1) return n;
  return fib(n - 1) + fib(n - 2);
}
```

이건 DP 문제를 **재귀적으로 푼 기본 예시**지. 하지만 효율이 안 좋지.

---

## **2. 재귀 + 메모이제이션 = Top-down DP**

**재귀로만 푸는 건 느릴 수 있음.**  
→ 그래서 **"메모이제이션(Memoization)"**을 결합해서 **중복 호출을 제거**해야 돼.

예시:
```js
const memo = {};

function fib(n) {
  if (n <= 1) return n;
  if (memo[n]) return memo[n];
  return memo[n] = fib(n - 1) + fib(n - 2);
}
```

이게 바로 **Top-down 방식의 DP**  
(재귀적으로 내려가되, 계산값을 저장하면서 올라옴)

---

## **3. 반복문 기반 DP = Bottom-up 방식**

```js
function fib(n) {
  const dp = [0, 1];
  for (let i = 2; i <= n; i++) {
    dp[i] = dp[i - 1] + dp[i - 2];
  }
  return dp[n];
}
```

- 재귀 없이 반복문으로 구현  
- **Stack Overflow 걱정 없음**  
- 일반적으로 **성능 더 좋고, 메모리도 적게 씀**

---

## **요약 비교**

| 방식             | 설명                          | 장단점                                  |
|------------------|-------------------------------|-------------------------------------------|
| **재귀**         | 단순 재귀, 중복 많음          | 느림, 스택 오버플로우 위험                 |
| **재귀 + 메모이제이션** | Top-down DP                 | 빠름, 재귀식 구조 유지, 가독성 ↑            |
| **반복문 DP**     | Bottom-up DP                  | 가장 빠름, 메모리 효율 좋음, 코드가 딱딱할 수 있음 |

---

## **결론**

- **모든 DP 문제 → 재귀 구조로 가능** (Top-down)
- 하지만 **시간/공간 효율** 때문에 대부분 **Bottom-up**으로 바꾸는 것
- 간단한 문제면 **재귀 + 메모이제이션**으로 푸는 게 훨씬 직관적이고 쉬워

---


</div>
</details>